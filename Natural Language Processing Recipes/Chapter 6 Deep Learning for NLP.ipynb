{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6 Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usages:<br>\n",
    "1. Search Engines\n",
    "2. Document Retrieval systems \n",
    "3. Passage Retrieval systems \n",
    "4. Question Answering Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1 = ['''With the Union cabinet approving the amendments to the\n",
    "Motor Vehicles Act, 2016, those caught for drunken driving will\n",
    "have to have really deep pockets, as the fine payable in court\n",
    "has been enhanced to Rs 10,000 for first-time offenders.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2 = ['''\"Natural language processing (NLP) is an area of\n",
    "computer science and artificial intelligence concerned with the\n",
    "interactions between computers and human (natural) languages,\n",
    "in particular how to program computers to process and analyze\n",
    "large amounts of natural language data.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc3 = ['''\"He points out that public transport is very good in\n",
    "Mumbai and New Delhi, where there is a good network of suburban\n",
    "and metro rail systems.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc4 = ['''But the man behind the wickets at the other end was\n",
    "watching just as keenly. With an affirmative nod from Dhoni,\n",
    "India captain Rohit Sharma promptly asked for a review. Sure\n",
    "enough, the ball would have clipped the top of middle and leg.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    #text = re.sub(r'[^\\w\\s]+',' ', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stp])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word):\n",
    "    if word in w2vec.vocab:\n",
    "        return w2vec[word]\n",
    "    else:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = Doc1+Doc2+Doc3+Doc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for se in fin:\n",
    "    average_vector = np.mean(np.array([get_embeddings(word) for word in nltk.word_tokenize(preprocess(se))]),axis=0)\n",
    "    d = {se:average_vector}\n",
    "    out_dict.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity between query and documents vectorsout_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(query, doc):\n",
    "    cos_sim = np.dot(query, doc)/(np.linalg.norm(query)*np.linalg.norm(doc))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate ranked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rank_text(query):\n",
    "    query_vector = np.mean(np.array([get_embeddings(word) for word in nltk.word_tokenize(preprocess(query))]),axis=0)\n",
    "    rank = []\n",
    "    for k,v in out_dict.items():\n",
    "        rank.append((k,get_similarity(query_vector, v)))\n",
    "    rank = sorted(rank, key=lambda x:x[1], reverse=True)\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('But the man behind the wickets at the other end was\\nwatching just as keenly. With an affirmative nod from Dhoni,\\nIndia captain Rohit Sharma promptly asked for a review. Sure\\nenough, the ball would have clipped the top of middle and leg.',\n",
       "  0.47628248),\n",
       " ('With the Union cabinet approving the amendments to the\\nMotor Vehicles Act, 2016, those caught for drunken driving will\\nhave to have really deep pockets, as the fine payable in court\\nhas been enhanced to Rs 10,000 for first-time offenders.',\n",
       "  0.2899310253890867),\n",
       " ('\"He points out that public transport is very good in\\nMumbai and New Delhi, where there is a good network of suburban\\nand metro rail systems.',\n",
       "  0.24180555),\n",
       " ('\"Natural language processing (NLP) is an area of\\ncomputer science and artificial intelligence concerned with the\\ninteractions between computers and human (natural) languages,\\nin particular how to program computers to process and analyze\\nlarge amounts of natural language data.',\n",
       "  0.19640841197922515)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rank_text('who was playing cricket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam ham classification from sms dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = file_content[['v2','v1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['features', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['features'],dataset['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize for word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode targets and convert to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = lbl.transform(y_train)\n",
    "y_test = lbl.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4457, 300), (1115, 300), (4457, 2), (1115, 2))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=MAX_NB_WORDS, output_dim=EMB_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(5),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    keras.layers.MaxPooling1D(5),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dense(units=2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 15s 221ms/step - loss: 0.4266 - accuracy: 0.8207 - val_loss: 0.5753 - val_accuracy: 0.8717\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 15s 213ms/step - loss: 0.1529 - accuracy: 0.9453 - val_loss: 0.6593 - val_accuracy: 0.8717\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 15s 211ms/step - loss: 0.0809 - accuracy: 0.9755 - val_loss: 0.4783 - val_accuracy: 0.8717\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 14s 206ms/step - loss: 0.0443 - accuracy: 0.9890 - val_loss: 0.2743 - val_accuracy: 0.8726\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 15s 207ms/step - loss: 0.0349 - accuracy: 0.9906 - val_loss: 0.2834 - val_accuracy: 0.9291\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64 ,epochs=5, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3016559 , 0.69834405],\n",
       "       [0.84343934, 0.15656064],\n",
       "       [0.825979  , 0.17402096],\n",
       "       ...,\n",
       "       [0.79840803, 0.20159195],\n",
       "       [0.7990837 , 0.2009163 ],\n",
       "       [0.83477   , 0.16523004]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       972\n",
      "           1       1.00      0.45      0.62       143\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1115\n",
      "   macro avg       0.96      0.72      0.79      1115\n",
      "weighted avg       0.93      0.93      0.92      1115\n",
      " samples avg       0.93      0.93      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=MAX_NB_WORDS, output_dim=EMB_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    keras.layers.SimpleRNN(units=2, input_shape=(None,1)),\n",
    "    keras.layers.Dense(units=2,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "140/140 [==============================] - 13s 93ms/step - loss: 0.2695 - accuracy: 0.9639 - val_loss: 0.1766 - val_accuracy: 0.9857\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 12s 87ms/step - loss: 0.1204 - accuracy: 0.9917 - val_loss: 0.1142 - val_accuracy: 0.9865\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 15s 105ms/step - loss: 0.0655 - accuracy: 0.9962 - val_loss: 0.0896 - val_accuracy: 0.9892\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 13s 94ms/step - loss: 0.0386 - accuracy: 0.9987 - val_loss: 0.0799 - val_accuracy: 0.9892\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 12s 88ms/step - loss: 0.0256 - accuracy: 0.9996 - val_loss: 0.0765 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 18ms/step - loss: 0.0765 - accuracy: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0764990895986557, 0.9847533702850342]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       972\n",
      "           1       1.00      0.88      0.94       143\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.99      0.98      0.98      1115\n",
      " samples avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=MAX_NB_WORDS, output_dim=EMB_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    keras.layers.LSTM(units=2, activation='relu', return_sequences=True),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=2,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "279/279 [==============================] - 35s 127ms/step - loss: 0.2326 - accuracy: 0.9181 - val_loss: 0.3325 - val_accuracy: 0.9525\n",
      "Epoch 2/5\n",
      "279/279 [==============================] - 34s 121ms/step - loss: 0.0657 - accuracy: 0.9789 - val_loss: 0.1144 - val_accuracy: 0.9686\n",
      "Epoch 3/5\n",
      "279/279 [==============================] - 33s 120ms/step - loss: 0.0335 - accuracy: 0.9888 - val_loss: 0.1058 - val_accuracy: 0.9677\n",
      "Epoch 4/5\n",
      "279/279 [==============================] - 34s 121ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.1144 - val_accuracy: 0.9722\n",
      "Epoch 5/5\n",
      "279/279 [==============================] - 34s 121ms/step - loss: 0.0120 - accuracy: 0.9953 - val_loss: 0.1255 - val_accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, batch_size=16, epochs=5, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       972\n",
      "           1       0.97      0.80      0.88       143\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1115\n",
      "   macro avg       0.97      0.90      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      " samples avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'spam.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained fashion mnist model on binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = tf.keras.models.load_model('my_model_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. form a modelB with all layers from modelA except output<br>\n",
    "add output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 265,701\n",
      "Trainable params: 265,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelB_on_A = keras.models.Sequential(modelA.layers[:-1])\n",
    "modelB_on_A.add(keras.layers.Dense(name='output',units=1,activation='sigmoid'))\n",
    "modelB_on_A.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Clone pretrained model to back up (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelA_clone = keras.models.clone_model(modelA)\n",
    "modelA_clone.set_weights(modelA.get_weights())\n",
    "modelA_clone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Freeze added layers of modelB for some epochs to avoid overriding of weights<br>\n",
    "After few epochs when output layers' weights are headed in right direction we can unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in modelB_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data for new task - binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test,y_test) = fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "category1_indices = np.where(y_test==0)[0]\n",
    "category2_indices = np.where(y_test==1)[0]\n",
    "X_cat1 = X_test[category1_indices][:100]\n",
    "X_cat2 = X_test[category2_indices][:100]\n",
    "X__train = np.concatenate([X_cat1,X_cat2],axis=0)\n",
    "y_cat1 = y_test[category1_indices][:100]\n",
    "y_cat2 = y_test[category2_indices][:100]\n",
    "y__train = np.concatenate([y_cat1,y_cat2],axis=0)\n",
    "X__test = np.concatenate([X_test[category1_indices][100:120],\n",
    "                         X_test[category2_indices][100:120]])\n",
    "y__test = np.concatenate([y_test[category1_indices][100:120],\n",
    "                         y_test[category2_indices][100:120]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fit the model with less epochs to let output layer adjust weights with freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 138.9800 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0040s). Check your callbacks.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1481.4801 - accuracy: 0.8400\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.4860 - accuracy: 0.9950\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.1019 - accuracy: 0.9900\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "history = modelB_on_A.fit(X__train,y__train,epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now unfreeze the layers to train weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in modelB_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Retrain the model with lower learning rate and more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.SGD(lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB_on_A.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4074 - accuracy: 0.9950\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1598.7640 - accuracy: 0.7400\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 13.1804 - accuracy: 0.9650\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2912 - accuracy: 0.9900\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8868e-10 - accuracy: 1.0000\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8868e-10 - accuracy: 1.0000\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.8868e-10 - accuracy: 1.0000\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.8871e-10 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = modelB_on_A.fit(X__train,y__train,epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Well Done, you performed transfer learning for a task !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am Great HaHa :p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
